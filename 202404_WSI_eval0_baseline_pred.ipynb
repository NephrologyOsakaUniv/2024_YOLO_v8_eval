{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247f98ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import glob, os, random, torch, shutil, cv2, h5py, joblib, gc, pickle, timm, concurrent.futures, json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from openslide import OpenSlide\n",
    "from PIL import Image, ImageEnhance, ImageOps, ImageDraw\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from tempfile import TemporaryDirectory\n",
    "from sklearn.metrics import precision_recall_curve, confusion_matrix\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, precision_recall_curve, average_precision_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = 'PCI_BUS_ID'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "seed = 0\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506b4670",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3cfa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfd46fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# クラス名の辞書\n",
    "class_names = {\n",
    "    0: \"Glo\",\n",
    "    1: \"GS\",\n",
    "    2: \"Cres\",\n",
    "    3: \"SegSc\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af2a4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 'base_PATH'\n",
    "models = sorted(glob.glob(d + '/PATH/*.pt'))\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c36c416",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = sorted(glob.glob(d + '/PATH/*/*.png'))\n",
    "print(len(files))\n",
    "print(files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2dfc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_predict(image_patch, is_top_edge=False, is_left_edge=False, is_bottom_edge=False, is_right_edge=False, YOLO_threshold=0.01, iou_threshold=0.5, image_size=640, edge_margin=10):\n",
    "    # YOLOモデルによる予測\n",
    "    results = model.predict(image_patch, save=False, conf=YOLO_threshold)\n",
    "    result_object = results[0]\n",
    "    bounding_boxes_prob_class = result_object.boxes.data.cpu().numpy()\n",
    "\n",
    "    # 辺縁から指定されたピクセル内にあるバウンディングボックスを条件付きで除外\n",
    "    filtered_boxes = []\n",
    "    for box in bounding_boxes_prob_class:\n",
    "        if (not is_top_edge and box[1] <= edge_margin) or \\\n",
    "           (not is_left_edge and box[0] <= edge_margin) or \\\n",
    "           (not is_bottom_edge and box[3] >= image_size - edge_margin) or \\\n",
    "           (not is_right_edge and box[2] >= image_size - edge_margin):\n",
    "            continue\n",
    "        filtered_boxes.append(box)\n",
    "\n",
    "    # 除外後のバウンディングボックスを返却\n",
    "    return np.array(filtered_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28472d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def YOLO_NMS(selected_f, SIZE=640):\n",
    "    Image.MAX_IMAGE_PIXELS = None\n",
    "    \n",
    "    try:\n",
    "        img = np.array(Image.open(selected_f))\n",
    "        height, width, _ = img.shape\n",
    "\n",
    "        all_coordinates = []\n",
    "        output_img_all_cord = img.copy()\n",
    "\n",
    "        for window_size in window_sizes:\n",
    "            stride = window_size[0] // 2\n",
    "\n",
    "            # 左上からパッチを切り出す処理                    \n",
    "            for y in range(0, height, stride):\n",
    "                for x in range(0, width, stride):\n",
    "                    x_end = min(width, x + window_size[0])\n",
    "                    y_end = min(height, y + window_size[1])\n",
    "                    image_patch = img[y:y_end, x:x_end]\n",
    "\n",
    "                    # 実際のパッチサイズを計算\n",
    "                    actual_patch_width = x_end - x\n",
    "                    actual_patch_height = y_end - y\n",
    "\n",
    "                    # スケーリング比率を計算\n",
    "                    scale_x = actual_patch_width / SIZE\n",
    "                    scale_y = actual_patch_height / SIZE\n",
    "\n",
    "                    image_patch = Image.fromarray(image_patch).resize((SIZE, SIZE), Image.LANCZOS)\n",
    "                    image_patch = np.array(image_patch)\n",
    "\n",
    "                    is_top_edge = y == 0\n",
    "                    is_left_edge = x == 0\n",
    "                    predictions = yolo_predict(image_patch, is_top_edge=is_top_edge, is_left_edge=is_left_edge)\n",
    "\n",
    "                    # 予測結果の処理\n",
    "                    for pred in predictions:\n",
    "                        x1, y1, x2, y2, score, cl = pred\n",
    "                        x1 = (x1 * scale_x) + x\n",
    "                        y1 = (y1 * scale_y) + y\n",
    "                        x2 = (x2 * scale_x) + x\n",
    "                        y2 = (y2 * scale_y) + y\n",
    "                        all_coordinates.append([x1, y1, x2, y2, score, cl])\n",
    "                        \n",
    "            # 右端の処理\n",
    "            for y in range(0, height - window_size[1] + 1, stride):\n",
    "                x = width - window_size[0]  # 右端からパッチを切り出すためのX座標\n",
    "                x_end = width  # 右端なので、x_endは画像の幅\n",
    "                y_end = y + window_size[1]  # y_endは通常通り\n",
    "                image_patch = img[y:y_end, x:x_end]  # パッチの切り出し\n",
    "\n",
    "                # スケーリング比率を計算（修正）\n",
    "                actual_patch_width = x_end - x  # 実際のパッチの幅\n",
    "                scale_x = actual_patch_width / SIZE\n",
    "                scale_y = window_size[1] / SIZE  # 高さは変わらない\n",
    "\n",
    "                image_patch = Image.fromarray(image_patch).resize((SIZE, SIZE), Image.LANCZOS)\n",
    "                image_patch = np.array(image_patch)\n",
    "\n",
    "                is_right_edge = True\n",
    "                predictions = yolo_predict(image_patch, is_right_edge=is_right_edge)\n",
    "\n",
    "                # 予測結果の処理\n",
    "                for pred in predictions:\n",
    "                    x1, y1, x2, y2, score, cl = pred\n",
    "                    x1 = (x1 * scale_x) + x  # xのオフセットを適用\n",
    "                    y1 = (y1 * scale_y) + y\n",
    "                    x2 = (x2 * scale_x) + x  # xのオフセットを適用\n",
    "                    y2 = (y2 * scale_y) + y\n",
    "                    all_coordinates.append([x1, y1, x2, y2, score, cl])\n",
    "                     \n",
    "            # 下端の処理\n",
    "            for x in range(0, width - window_size[0] + 1, stride):\n",
    "                y = height - window_size[1]  # 下端からパッチを切り出すためのY座標\n",
    "                x_end = x + window_size[0]  # x_endは通常通り\n",
    "                y_end = height  # 下端なので、y_endは画像の高さ\n",
    "                image_patch = img[y:y_end, x:x_end]  # パッチの切り出し\n",
    "\n",
    "                # スケーリング比率を計算（修正）\n",
    "                actual_patch_height = y_end - y  # 実際のパッチの高さ\n",
    "                scale_x = window_size[0] / SIZE  # 幅は変わらない\n",
    "                scale_y = actual_patch_height / SIZE\n",
    "\n",
    "                image_patch = Image.fromarray(image_patch).resize((SIZE, SIZE), Image.LANCZOS)\n",
    "                image_patch = np.array(image_patch)\n",
    "\n",
    "                is_bottom_edge = True\n",
    "                predictions = yolo_predict(image_patch, is_bottom_edge=is_bottom_edge)\n",
    "\n",
    "                # 予測結果の処理\n",
    "                for pred in predictions:\n",
    "                    x1, y1, x2, y2, score, cl = pred\n",
    "                    x1 = (x1 * scale_x) + x\n",
    "                    y1 = (y1 * scale_y) + y  # yのオフセットを適用\n",
    "                    x2 = (x2 * scale_x) + x\n",
    "                    y2 = (y2 * scale_y) + y  # yのオフセットを適用\n",
    "                    all_coordinates.append([x1, y1, x2, y2, score, cl])\n",
    "\n",
    "        # 予測されたすべてのバウンディングボックスを黒四角で描画\n",
    "        for coord in all_coordinates:\n",
    "            x1, y1, x2, y2, score, cl = map(int, coord)\n",
    "            cv2.rectangle(output_img_all_cord, (x1, y1), (x2, y2), (0, 0, 0), 2)\n",
    "\n",
    "        # 保存処理            \n",
    "        all_coordinates_np = np.array(all_coordinates)\n",
    "        np.save(os.path.splitext(selected_f)[0] + '_weight_'+ os.path.basename(os.path.splitext(mo)[0]) +'_0_bbox_coordinates.npy', all_coordinates_np)  \n",
    "        cv2.imwrite(os.path.splitext(selected_f)[0] + '_'+ os.path.basename(os.path.splitext(mo)[0]) + '_0_output_image.png', \n",
    "                    np.array(Image.fromarray(output_img_all_cord).resize((output_img_all_cord.shape[1]//2, output_img_all_cord.shape[0]//2), \n",
    "                                                                         Image.LANCZOS)))\n",
    "    \n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {selected_f}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d26201c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "window_sizes = [(2048, 2048), (1536, 1536), (1024, 1024)]\n",
    "\n",
    "for mo in models:\n",
    "    model = YOLO(mo)\n",
    "    result = joblib.Parallel(n_jobs=16, verbose = 1)(joblib.delayed(YOLO_NMS)(selected_f) for selected_f in files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
